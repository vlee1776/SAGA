{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A familiar dataset which I've played with and have parsed, needs to be changed to the other datasets which the paper uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: (768, 8)\n",
      "Shape of y: (768,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy\n",
    "\n",
    "x_sparse, y = datasets.load_svmlight_file('diabetes')\n",
    "x = x_sparse.todense()\n",
    "\n",
    "print('Shape of x: ' + str(x.shape))\n",
    "print('Shape of y: ' + str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (640, 8)\n",
      "Shape of x_test: (128, 8)\n",
      "Shape of y_train: (640, 1)\n",
      "Shape of y_test: (128, 1)\n"
     ]
    }
   ],
   "source": [
    "# partition the data to training and test sets\n",
    "n = x.shape[0]\n",
    "n_train = 640\n",
    "n_test = n - n_train\n",
    "\n",
    "rand_indices = numpy.random.permutation(n)\n",
    "train_indices = rand_indices[0:n_train]\n",
    "test_indices = rand_indices[n_train:n]\n",
    "\n",
    "x_train = x[train_indices, :]\n",
    "x_test = x[test_indices, :]\n",
    "y_train = y[train_indices].reshape(n_train, 1)\n",
    "y_test = y[test_indices].reshape(n_test, 1)\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape))\n",
    "print('Shape of x_test: ' + str(x_test.shape))\n",
    "print('Shape of y_train: ' + str(y_train.shape))\n",
    "print('Shape of y_test: ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mean = \n",
      "[[-0.00323299  0.04022588  0.0292436   0.05085141  0.03199379  0.03404022\n",
      "   0.17395318  0.05570114]]\n",
      "test std = \n",
      "[[0.97180563 0.93688322 1.08138887 1.00845326 1.19543904 0.90443632\n",
      "  0.92388706 1.05479545]]\n"
     ]
    }
   ],
   "source": [
    "# Standardization\n",
    "import numpy\n",
    "\n",
    "# calculate mu and sig using the training set\n",
    "d = x_train.shape[1]\n",
    "mu = numpy.mean(x_train, axis=0).reshape(1, d)\n",
    "sig = numpy.std(x_train, axis=0).reshape(1, d)\n",
    "\n",
    "# transform the training features\n",
    "x_train = (x_train - mu) / (sig + 1E-6)\n",
    "\n",
    "# transform the test features\n",
    "x_test = (x_test - mu) / (sig + 1E-6)\n",
    "\n",
    "print('test mean = ')\n",
    "print(numpy.mean(x_test, axis=0))\n",
    "\n",
    "print('test std = ')\n",
    "print(numpy.std(x_test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (640, 9)\n",
      "Shape of x_test: (128, 9)\n"
     ]
    }
   ],
   "source": [
    "n_train, d = x_train.shape\n",
    "x_train = numpy.concatenate((x_train, numpy.ones((n_train, 1))), axis=1)\n",
    "\n",
    "n_test, d = x_test.shape\n",
    "x_test = numpy.concatenate((x_test, numpy.ones((n_test, 1))), axis=1)\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape))\n",
    "print('Shape of x_test: ' + str(x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Loss Objective Value\n",
    "\n",
    "Objective Function:\n",
    "$ q (w; x, y) = \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big)$ \n",
    "\n",
    "l2 regularizer addition:\n",
    "$\\frac{\\lambda}{2} \\| w \\|_2^2 $.\n",
    "\n",
    "l1 regularizer addition: \n",
    "$\\lambda| w \\|^1 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y: scalar \n",
    "# x: 1 by d vector \n",
    "# w: d by 1 vector\n",
    "# lam: scalar \n",
    "# l1 and l2: boolean\n",
    "def logistic_loss(y,x,w,l1=False,l2=False,lam=0):\n",
    "    exponent = float(numpy.exp(-y * numpy.dot(x,w))) # scalar\n",
    "    objective = numpy.log(1 + exponent) \n",
    "    if l2:\n",
    "        objective += lam/2 * numpy.sum(numpy.multiply(w,w))\n",
    "    if l1:\n",
    "        objective += numpy.sum(numpy.abs(w)) \n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient of logisitic loss\n",
    "# y: scalar \n",
    "# x: 1 by d vector \n",
    "# w: d by 1 vector\n",
    "# lam: scalar \n",
    "# l1 and l2: boolean\n",
    "def logistic_gradient(y,x,w,l1=False,l2=False, lam=0):\n",
    "    d, _ = w.shape\n",
    "    exponent = float(numpy.exp(y * numpy.dot(x,w))) # scalar\n",
    "    derivative = (-y * x) / (1 + exponent) \n",
    "    derivative = derivative.reshape((d,1))# d by 1\n",
    "    if l2:\n",
    "        derivative += lam * w\n",
    "    if l1:\n",
    "        derivative += numpy.multiply(numpy.sign(w),w)\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: write equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y: scalar \n",
    "# x: 1 by d vector \n",
    "# w: d by 1 vector\n",
    "# lam: scalar \n",
    "# l1 and l2: boolean\n",
    "def linear_loss(y,x,w,l2=False,l1=False,lam=0):\n",
    "    loss = numpy.dot(x,w) - y\n",
    "    loss = numpy.multiply(loss,loss)\n",
    "    if l2:\n",
    "        loss += lam * numpy.multiply(w,w)\n",
    "    if l1:\n",
    "        loss += lam * numpy.sum(numpy.abs(w))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y: scalar \n",
    "# x: 1 by d vector \n",
    "# w: d by 1 vector\n",
    "# lam: scalar \n",
    "# l1 and l2: boolean\n",
    "def linear_gradient(y,x,w,l2=False,l1=False,lam=0):\n",
    "    d, _ = w.shape\n",
    "    scalar = numpy.dot(w,x) - y\n",
    "    vector = scalar * x \n",
    "    vector.reshape((d,1))\n",
    "    if l2:\n",
    "        vector += lam * w\n",
    "    if l1:\n",
    "        vector += numpy.multiply(numpy.sign(w),w)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAGA Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: training set, n by d \n",
    "# y: training labels, n by 1 \n",
    "# step_size, can't go wrong with 1/3L\n",
    "# lam: scalar\n",
    "# max_epochs = scalar\n",
    "# proximal: not always used default should be lambda x\n",
    "# obj_func: R^d -> R^1\n",
    "# grad_func: R^d -> R^d\n",
    "# l1 and l2 = Booleans\n",
    "def saga(X,y,step_size,max_epochs,proximal,obj_func,grad_func,l1=False,l2=False,lam=0):\n",
    "    # average obj values per epoch\n",
    "    obj_vals = []\n",
    "    n, d = X.shape  \n",
    "    w = numpy.zeros((d,1)) # d by 1 weight vector\n",
    "    # initialize table with derivative w/weight 0\n",
    "    derivatives = numpy.zeros((n,d))\n",
    "    for i in range(d):\n",
    "        derivatives[i,:] = grad_func(y[i],X[i,:],w,l1,l2,lam).reshape(9)\n",
    "        \n",
    "    for epoch in range(max_epochs):\n",
    "        # shuffle data points for an epoch\n",
    "        permutation = numpy.random.permutation(n)\n",
    "        X_shuffled = X[permutation,:]\n",
    "        y_shuffled = y[permutation,:]\n",
    "        obj_epoch = 0\n",
    "        for i in range(n):\n",
    "            # target data point and label\n",
    "            xi = X_shuffled[i,:]\n",
    "            yi = y_shuffled[i]\n",
    "\n",
    "            updated_deriv = grad_func(yi,xi,w,l1=l1,l2=l2,lam=lam) # d by 1\n",
    "            previous_deriv = derivatives[permutation[i],:].reshape((d,1)) # d by 1\n",
    "            derivatives[permutation[i],:] = updated_deriv.reshape(d)\n",
    "            table_avg = numpy.mean(derivatives,axis=0).reshape((d,1))\n",
    "            update = updated_deriv - previous_deriv + table_avg\n",
    "            w = w - (step_size * update)\n",
    "            # apply proximal operator\n",
    "            w = proximal(w)\n",
    "            obj_iter = obj_func(y_shuffled[i],X_shuffled[i,:],w,l1=l1,l2=l2,lam=lam)\n",
    "            obj_epoch += obj_iter\n",
    "\n",
    "        obj_epoch /= n\n",
    "        obj_vals.append(obj_epoch)\n",
    "        print(\"Obj val at epoch \" + str(epoch) + ' is ' + str(obj_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj val at epoch 0 is 0.6588002630514876\n",
      "Obj val at epoch 1 is 0.5515855680421203\n",
      "Obj val at epoch 2 is 0.5104804259070631\n",
      "Obj val at epoch 3 is 0.49584571218481754\n",
      "Obj val at epoch 4 is 0.4885791791052088\n",
      "Obj val at epoch 5 is 0.48431571830607245\n",
      "Obj val at epoch 6 is 0.4818573108614399\n",
      "Obj val at epoch 7 is 0.4801292759831207\n",
      "Obj val at epoch 8 is 0.4792396018693451\n",
      "Obj val at epoch 9 is 0.47820711633892243\n",
      "Obj val at epoch 10 is 0.47778266331929514\n",
      "Obj val at epoch 11 is 0.47763266457157705\n",
      "Obj val at epoch 12 is 0.4773093432078349\n",
      "Obj val at epoch 13 is 0.4772009244614339\n",
      "Obj val at epoch 14 is 0.4770749871833301\n",
      "Obj val at epoch 15 is 0.477013897631785\n",
      "Obj val at epoch 16 is 0.4769733361655414\n",
      "Obj val at epoch 17 is 0.47687696721618406\n",
      "Obj val at epoch 18 is 0.47686376320372653\n",
      "Obj val at epoch 19 is 0.4768044854767887\n",
      "Obj val at epoch 20 is 0.4767733835220655\n",
      "Obj val at epoch 21 is 0.47677463436519807\n",
      "Obj val at epoch 22 is 0.4767707909431412\n",
      "Obj val at epoch 23 is 0.4767659052698935\n",
      "Obj val at epoch 24 is 0.476760208733813\n",
      "Obj val at epoch 25 is 0.4767449305308934\n",
      "Obj val at epoch 26 is 0.47673922896444737\n",
      "Obj val at epoch 27 is 0.4767275610072457\n",
      "Obj val at epoch 28 is 0.4767264041599886\n",
      "Obj val at epoch 29 is 0.47672401998418196\n",
      "Obj val at epoch 30 is 0.4767263158707113\n",
      "Obj val at epoch 31 is 0.4767261074795332\n",
      "Obj val at epoch 32 is 0.4767262671602824\n",
      "Obj val at epoch 33 is 0.4767229216595668\n",
      "Obj val at epoch 34 is 0.4767269832076094\n",
      "Obj val at epoch 35 is 0.4767259315335518\n",
      "Obj val at epoch 36 is 0.47672292127019744\n",
      "Obj val at epoch 37 is 0.4767268962776793\n",
      "Obj val at epoch 38 is 0.4767259843797634\n",
      "Obj val at epoch 39 is 0.47672442929339764\n",
      "Obj val at epoch 40 is 0.4767258684349157\n",
      "Obj val at epoch 41 is 0.47672396509082865\n",
      "Obj val at epoch 42 is 0.47672324013998935\n",
      "Obj val at epoch 43 is 0.47672264135434544\n",
      "Obj val at epoch 44 is 0.4767226659721362\n",
      "Obj val at epoch 45 is 0.4767249710601688\n",
      "Obj val at epoch 46 is 0.47672465877717884\n",
      "Obj val at epoch 47 is 0.47672445895190696\n",
      "Obj val at epoch 48 is 0.47672512513998055\n",
      "Obj val at epoch 49 is 0.4767248129798052\n",
      "Obj val at epoch 50 is 0.4767242338171963\n",
      "Obj val at epoch 51 is 0.47672416839110376\n",
      "Obj val at epoch 52 is 0.4767242267043253\n",
      "Obj val at epoch 53 is 0.47672444732679364\n",
      "Obj val at epoch 54 is 0.4767238386944159\n",
      "Obj val at epoch 55 is 0.47672406230460823\n",
      "Obj val at epoch 56 is 0.4767240920049562\n",
      "Obj val at epoch 57 is 0.4767240285147734\n",
      "Obj val at epoch 58 is 0.47672425192118456\n",
      "Obj val at epoch 59 is 0.47672415245555655\n",
      "Obj val at epoch 60 is 0.4767241364166138\n",
      "Obj val at epoch 61 is 0.4767243065322228\n",
      "Obj val at epoch 62 is 0.47672421799251563\n",
      "Obj val at epoch 63 is 0.4767241656135083\n",
      "Obj val at epoch 64 is 0.47672423155574845\n",
      "Obj val at epoch 65 is 0.4767242463018569\n",
      "Obj val at epoch 66 is 0.4767242561266659\n",
      "Obj val at epoch 67 is 0.4767241759509936\n",
      "Obj val at epoch 68 is 0.47672420281279876\n",
      "Obj val at epoch 69 is 0.47672417632731234\n",
      "Obj val at epoch 70 is 0.4767242115538057\n",
      "Obj val at epoch 71 is 0.4767241261589453\n",
      "Obj val at epoch 72 is 0.4767241659668346\n",
      "Obj val at epoch 73 is 0.476724198318771\n",
      "Obj val at epoch 74 is 0.4767241825061693\n",
      "Obj val at epoch 75 is 0.47672418940750727\n",
      "Obj val at epoch 76 is 0.47672419550611433\n",
      "Obj val at epoch 77 is 0.4767241960956592\n",
      "Obj val at epoch 78 is 0.47672419384868947\n",
      "Obj val at epoch 79 is 0.47672419823983725\n",
      "Obj val at epoch 80 is 0.47672421231194717\n",
      "Obj val at epoch 81 is 0.4767241922911241\n",
      "Obj val at epoch 82 is 0.47672419317630765\n",
      "Obj val at epoch 83 is 0.4767241931555694\n",
      "Obj val at epoch 84 is 0.47672418500837965\n",
      "Obj val at epoch 85 is 0.4767241894223324\n",
      "Obj val at epoch 86 is 0.47672419009524836\n",
      "Obj val at epoch 87 is 0.47672419902212076\n",
      "Obj val at epoch 88 is 0.4767241993736728\n",
      "Obj val at epoch 89 is 0.47672419139168776\n",
      "Obj val at epoch 90 is 0.4767241919045951\n",
      "Obj val at epoch 91 is 0.4767241926459092\n",
      "Obj val at epoch 92 is 0.47672419892786816\n",
      "Obj val at epoch 93 is 0.47672419563220314\n"
     ]
    }
   ],
   "source": [
    "proximal = lambda x: x\n",
    "_,eigs,_ = numpy.linalg.svd(x_train * x_train.T)\n",
    "n,d = numpy.shape(x_train)\n",
    "alpha = 1E-6\n",
    "step_size = 1 / (n * alpha + 1/4 * eigs[0]) \n",
    "saga(x_train,y_train,step_size,100,proximal,logistic_loss,logistic_gradient,l2=True,lam=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
